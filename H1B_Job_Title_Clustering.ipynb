{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clustering to clean H1B Job Titles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In my analytics class, we learned how to use Google's OpenRefine to clean H1B Visa applications and their job title fields. We used the clustering functionality, which was super cool and made me wonder how the heck Google did that stuff. This project is an attempt to replicate the functionality to some degree. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's import the necessary packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "\n",
    "import sklearn.cluster\n",
    "import distance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data is taken from governmental records of applications for HB1 visas. You can find it here: http://www.flcdatacenter.com/caseh1b.aspx. I saved it with a csv extension in Sublime."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's load the data into a dataframe so we can steal the column we want."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SUBMITTED_DATE           object\n",
       "CASE_NO                  object\n",
       "NAME                     object\n",
       "ADDRESS                  object\n",
       "ADDRESS2                 object\n",
       "CITY                     object\n",
       "STATE                    object\n",
       "POSTAL_CODE              object\n",
       "NBR_IMMIGRANTS            int64\n",
       "BEGIN_DATE               object\n",
       "END_DATE                 object\n",
       "JOB_TITLE                object\n",
       "DOL_DECISION_DATE        object\n",
       "CERTIFIED_BEGIN_DATE     object\n",
       "CERTIFIED_END_DATE       object\n",
       "JOB_CODE                  int64\n",
       "APPROVAL_STATUS          object\n",
       "WAGE_RATE_1             float64\n",
       "RATE_PER_1               object\n",
       "MAX_RATE_1              float64\n",
       "PART_TIME_1              object\n",
       "CITY_1                   object\n",
       "STATE_1                  object\n",
       "PREVAILING_WAGE_1       float64\n",
       "WAGE_SOURCE_1            object\n",
       "YR_SOURCE_PUB_1         float64\n",
       "OTHER_WAGE_SOURCE_1      object\n",
       "WAGE_RATE_2             float64\n",
       "RATE_PER_2               object\n",
       "MAX_RATE_2              float64\n",
       "PART_TIME_2              object\n",
       "CITY_2                   object\n",
       "STATE_2                  object\n",
       "PREVAILING_WAGE_2       float64\n",
       "WAGE_SOURCE_2            object\n",
       "YR_SOURCE_PUB_2         float64\n",
       "OTHER_WAGE_SOURCE_2      object\n",
       "dtype: object"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"H1B.csv\")\n",
    "df.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're going to be working with job titles. First, let's take a look at what our job titles look like so we can understand the problem. The distance formula we're going to use is a bit computationally expensive, we'll only use the first 300 jobs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "titles = pd.Series(df['JOB_TITLE'].unique())[:300]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                  MOLECULAR BIOLOGIST\n",
       "1          Principal Software Engineer\n",
       "2       Associate Sales & Distribution\n",
       "3                   resident physician\n",
       "4              Market Research Analyst\n",
       "5                        fellow doctor\n",
       "6                      Systems Analyst\n",
       "7                             Lecturer\n",
       "8                       Sales Engineer\n",
       "9                     Business Analyst\n",
       "10             Radiologic Technologist\n",
       "11                 Account Executive I\n",
       "12    Imaging Business Support Manager\n",
       "13                   Software Engineer\n",
       "14                 Assistant Professor\n",
       "15          Senior Computer Programmer\n",
       "16                 Industrial Engineer\n",
       "17                   MARINE TECHNICIAN\n",
       "18                  Programmer Analyst\n",
       "19             Postgraduate Researcher\n",
       "dtype: object"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "titles.head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see if you scroll through the list, even once we've taken the unique titles out of the dataframe, there are overlapping positions. There are lower case and upper case, words switched around, misspellings, etc. If we want to make this data useful and visualize it, we'll need to clean this up."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we can change all of the terms to lowercase. We can argue that it's a good idea to keep the punctuation, but we'll remove it to make it easier on the clustering later on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Convert all titles to lower case\n",
    "titles = titles.apply(lambda x: x.lower())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, now our data is ready for clustering."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One way to calculate the similarity between strings is called the \"Levenshtein Distance.\" Code borrowed from http://stats.stackexchange.com/questions/123060/clustering-a-long-list-of-strings-words-into-similarity-groups. More info on the distance formula here: https://rosettacode.org/wiki/Levenshtein_distance#Python. This may take a few minutes to run. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "lev_similarity = -1 * np.array([[distance.levenshtein(j1,j2) for j1 in titles] for j2 in titles])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we've created a matrix (in array form) of the Levenshtein Distance of each job title from the other job titles in the original jobs array. Here's what it looks like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  0, -22, -24, ..., -17, -17, -14],\n",
       "       [-22,   0, -25, ..., -22, -14, -23],\n",
       "       [-24, -25,   0, ..., -25, -23, -25],\n",
       "       ..., \n",
       "       [-17, -22, -25, ...,   0, -15, -15],\n",
       "       [-17, -14, -23, ..., -15,   0, -14],\n",
       "       [-14, -23, -25, ..., -15, -14,   0]])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lev_similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we'll cluster these values. Affinity Propagation seems to be the right algorithm for the job, since we've already calculated the Levenshtein Distances for our jobs array. The algorithm was first proposed for this purpose here:http://science.sciencemag.org/content/315/5814/972."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Affinity Propagation seems similar to K-Means, but instead of clustering and then re-iterating, the algorithm sends messages from data to other data to figure out what's close and what's not. K-Means, on the other hand, chooses random centroids (not the case in AP) and then figures out which points are closest. Info from here: http://www.psi.toronto.edu/affinitypropagation/faq.html."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another super important (and helpful) feature of Affinity Propagation is that we don't need to specify the number of centroids / exemplars, which is key for the nature of our data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AffinityPropagation(affinity='precomputed', convergence_iter=15, copy=True,\n",
       "          damping=0.5, max_iter=200, preference=None, verbose=False)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "affprop = sklearn.cluster.AffinityPropagation(affinity=\"precomputed\", damping = 0.5)\n",
    "affprop.fit(lev_similarity)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we fit the model, let's print out all of the clusters into a Pandas series (so we can index by a string)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def orderClusters(array):\n",
    "    \n",
    "    clusters = pd.Series()\n",
    "    \n",
    "    for cluster_id in np.unique(affprop.labels_):\n",
    "\n",
    "        exemplar = array[affprop.cluster_centers_indices_[cluster_id]]\n",
    "\n",
    "        cluster = np.unique(array[affprop.labels_==cluster_id])\n",
    "\n",
    "        if exemplar not in clusters:\n",
    "            clusters[exemplar] = cluster\n",
    "            \n",
    "    return clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "software engineer :  ['computer software engineer' 'housekeeper general'\n",
      " 'principal software engineer' 'software developer'\n",
      " 'software development engineer' 'software engineer'\n",
      " 'software engineer (associate)' 'software engineer vii'\n",
      " 'software engineering manager' 'staff software engineer'\n",
      " 'study abroad advisor']\n",
      "assistant professor :  ['adjunct/visiting professor' 'assistant coordinator' 'assistant professor'\n",
      " 'assistant professor of mathematics' 'assistant professor of philosophy'\n",
      " 'foreign student advisor' 'res assistant plastic surgery'\n",
      " 'sales manager/data processing' 'visiting assistant professor'\n",
      " 'visiting professor']\n",
      "marine technician :  ['dental laboratory technician' 'engineering technician'\n",
      " 'helpers electricians' 'marine technician' 'research technician'\n",
      " 'resident physician' 'staff physician']\n",
      "business development manager :  ['business area controller' 'business development manager'\n",
      " 'imaging business support manager' 'product development scientist']\n",
      "human resources director :  ['human resource manager' 'human resources director'\n",
      " 'human resources specialist' 'marketing procurrent director']\n",
      "research associate pediatrics :  ['res associate materials science' 'research associate pediatrics'\n",
      " 'research associate physics' 'research assoicate  pediatrics']\n",
      "assistant research scientist :  ['assist. project  scientist' 'assistant research linguist'\n",
      " 'assistant research scientist' 'molecular biology research scientist '\n",
      " 'postgraduate researcher scientist']\n",
      "systems analyst configuration specialist  :  ['systems analyst configuration specialist ']\n",
      "programmer analyst :  ['administrative analyst' 'global commodities analyst'\n",
      " 'infrastructure analyst' 'programmer' 'programmer analyst'\n",
      " 'programmer/analyst']\n",
      "postdoctoral research associate :  ['post doctoral research associate' 'post-doctoral research associate'\n",
      " 'postdoctoral research associate' 'postdoctoral research assoicate'\n",
      " 'postdoctoral research trainee']\n"
     ]
    }
   ],
   "source": [
    "clusters = orderClusters(titles)\n",
    "\n",
    "counter = 0\n",
    "for key in clusters.keys():\n",
    "    print key , ': ' , clusters[key]\n",
    "    counter += 1\n",
    "    if counter >= 10:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Definitely not bad for a first run through! A lot of these make sense and probably caught some real errors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we adjust the \"preference\" argument, we can force the algorithm to employ more clusters. To see how changing the preference changes our cluster size, we'll run it a few different times to find the length of the dictionary (i.e. the number of exemplars)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "154"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "affprop = sklearn.cluster.AffinityPropagation(affinity=\"precomputed\", damping = 0.5, preference = -10)\n",
    "affprop.fit(lev_similarity)\n",
    "clusters = orderClusters(titles)\n",
    "len(clusters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "263"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "affprop = sklearn.cluster.AffinityPropagation(affinity=\"precomputed\", damping = 0.5, preference = -3)\n",
    "affprop.fit(lev_similarity)\n",
    "clusters = orderClusters(titles)\n",
    "len(clusters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The total number of data points that we grabbed is close to being reached, so we can see that as the preference approaches 0, we approach no clusters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's print out all of the clusters that have more than one member."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "research assoicate  pediatrics :  ['research associate pediatrics' 'research assoicate  pediatrics']\n",
      "programmer analyst :  ['programmer analyst' 'programmer/analyst']\n",
      "postdoctoral research associate :  ['post doctoral research associate' 'post-doctoral research associate'\n",
      " 'postdoctoral research associate' 'postdoctoral research assoicate']\n",
      "research assistant :  ['research assistant' 'research assistant i' 'research associate']\n",
      "senior analyst :  ['junior analyst' 'senior analyst']\n",
      "computer programmer :  ['computer programer' 'computer programmer']\n",
      "chemical engineer :  ['chemical engineer' 'electrical engineer' 'mechanical engineer']\n",
      "technical specialist :  ['financial specialist' 'technical specialist']\n",
      "post doctoral researcher :  ['post doctoral researcher' 'postdoctoral researcher']\n",
      "architect :  ['architect' 'architect ']\n",
      "manager computer operations :  ['manager compouter operations' 'manager computer operations']\n",
      "associate director,  financial :  ['associate director,  financial' 'associate director, financial']\n",
      "teacher, speech/hearing hadicapped :  ['teacher, speech/hearing hadicapped' 'teacher, speech/hearing handicapped']\n",
      "product manager :  ['product manager' 'project manager']\n",
      "it analyst :  ['analyst' 'it analyst']\n",
      "computer programmer - i :  ['computer programmer - i' 'computer programmer - ii'\n",
      " 'computer programmer - iv']\n"
     ]
    }
   ],
   "source": [
    "clusters = orderClusters(titles)\n",
    "\n",
    "for key in clusters.keys():\n",
    "    if len(clusters[key]) >= 2:\n",
    "        print key , ': ' , clusters[key]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So how the heck are we supposed to know how many clusters are correct = what preference to use? Well, that's a great question, and the subject of this exact research paper from Cornell: https://arxiv.org/abs/0805.1096. The basic idea is – keep iterating until you converge on the right amount of clusters. It's called \"Adaptive Affinity Propagation.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Conclusions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1) To improve the accuracy of this project, we'd need to implement and adaptive framework to find the right preference value (= the optimal number of clusters)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2) For full effectiveness, we'd need to run the distance formula and clustering algorithm on every group, which would require some distributed computing on a cluster."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
