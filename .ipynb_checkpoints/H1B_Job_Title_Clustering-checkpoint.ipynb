{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clustering to clean H1B Job Titles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal of this project is to try to replicate the clustering functionality of Google's OpenRefine software. The idea is that in some data fields, unstructured entries that are spelled differently, etc., may really mean the same thing. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's import the necessary packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import sklearn.cluster\n",
    "import distance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data is taken from governmental records of applications for HB1 visas. You can find it here: https://nyu.app.box.com/s/9oz3qx886zpwwfm6ewj89pvjuee2eqp5. I saved it with a csv extension in Sublime."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's load the data into a dataframe so we can steal the column we want."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SUBMITTED_DATE           object\n",
       "CASE_NO                  object\n",
       "NAME                     object\n",
       "ADDRESS                  object\n",
       "ADDRESS2                 object\n",
       "CITY                     object\n",
       "STATE                    object\n",
       "POSTAL_CODE              object\n",
       "NBR_IMMIGRANTS            int64\n",
       "BEGIN_DATE               object\n",
       "END_DATE                 object\n",
       "JOB_TITLE                object\n",
       "DOL_DECISION_DATE        object\n",
       "CERTIFIED_BEGIN_DATE     object\n",
       "CERTIFIED_END_DATE       object\n",
       "JOB_CODE                  int64\n",
       "APPROVAL_STATUS          object\n",
       "WAGE_RATE_1             float64\n",
       "RATE_PER_1               object\n",
       "MAX_RATE_1              float64\n",
       "PART_TIME_1              object\n",
       "CITY_1                   object\n",
       "STATE_1                  object\n",
       "PREVAILING_WAGE_1       float64\n",
       "WAGE_SOURCE_1            object\n",
       "YR_SOURCE_PUB_1         float64\n",
       "OTHER_WAGE_SOURCE_1      object\n",
       "WAGE_RATE_2             float64\n",
       "RATE_PER_2               object\n",
       "MAX_RATE_2              float64\n",
       "PART_TIME_2              object\n",
       "CITY_2                   object\n",
       "STATE_2                  object\n",
       "PREVAILING_WAGE_2       float64\n",
       "WAGE_SOURCE_2            object\n",
       "YR_SOURCE_PUB_2         float64\n",
       "OTHER_WAGE_SOURCE_2      object\n",
       "dtype: object"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"H1B.csv\")\n",
    "df.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're going to be working with job titles. First, let's take a look at what our job titles look like so we can understand the problem. We'll group the dataframe by titles, and then extract each one to a numpy array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "titles = df.groupby('JOB_TITLE')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "jobs = []\n",
    "counter = 0\n",
    "\n",
    "for group in titles.groups:\n",
    "    if group not in jobs:\n",
    "        jobs.append(group)\n",
    "        counter += 1\n",
    "    if counter >= 500:\n",
    "        break\n",
    "\n",
    "jobs_array = np.asarray(jobs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Software Engineer (Consultant)',\n",
       "       'Software Engineer (Software Development Director)',\n",
       "       'Assistant VP - Economist',\n",
       "       'VICE PRESIDENT & CHIEF OPERATING OFFICER', 'PHYSICIAN RESIDENT',\n",
       "       'Network Manager', 'IT Architect', 'BUSINESS DEVELOPMENT MANAGER ',\n",
       "       'PostDoctoral Fellow', 'Adjunct Trainer',\n",
       "       'PATENT SPECIALIST(Chemical Arts)', 'Staff Research Associate',\n",
       "       'SR. FINANCIAL TECHNOLOGY ADVISOR', 'Software Project Engineer',\n",
       "       'COMPUTER SUPPORT SPECIALIST', 'PGY 4 Medical Resident/Fellow ',\n",
       "       'DENTAL OFFICE MANAGER AND DENTAL ASSISTANT', 'PROGAMMER ANALYST',\n",
       "       'Web Applications Developer', 'Computer Systems Administrator'], \n",
       "      dtype='|S50')"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jobs_array[:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see if you scroll through the list, even once we've taken the unique titles out of the dataframe, there are tons of overlapping positions. There are lower case and upper case, words switched around, misspellings, etc. If we want to make this data useful and visualize it, we'll need to clean this up."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we can change all of the terms to lowercase. We can argue that it's a good idea to keep the punctuation, but we'll remove it to make it easier on the clustering later on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Convert all titles to lower case\n",
    "\n",
    "for i in range(len(jobs_array)):\n",
    "    jobs_array[i] = jobs_array[i].lower()\n",
    "\n",
    "#Strip punctuation\n",
    "\n",
    "for i in range(len(jobs_array)):\n",
    "    jobs_array[i] = jobs_array[i].strip('/.,:;-– ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, now our data is ready for clustering."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The way to calculate the similarity between strings is called the \"Levenshtein Distance.\" Code borrowed from http://stats.stackexchange.com/questions/123060/clustering-a-long-list-of-strings-words-into-similarity-groups. More info on the distance formula here: https://rosettacode.org/wiki/Levenshtein_distance#Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "lev_similarity = -1 * np.array([[distance.levenshtein(j1,j2) for j1 in jobs_array] for j2 in jobs_array])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we've created a matrix (in array form) of the Levenshtein Distance of each job title from the other job titles in the original jobs array. Here's what it looks like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  0, -25, -23, ..., -19, -18, -28],\n",
       "       [-25,   0, -40, ..., -38, -36, -37],\n",
       "       [-23, -40,   0, ..., -17, -18, -22],\n",
       "       ..., \n",
       "       [-19, -38, -17, ...,   0, -12, -24],\n",
       "       [-18, -36, -18, ..., -12,   0, -25],\n",
       "       [-28, -37, -22, ..., -24, -25,   0]])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lev_similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we'll cluster these values. Affinity Propagation seems to be the right algorithm for the job, since we've already calculated the Levenshtein Distances for our jobs array. The algorithm was first proposed for this purpose here:http://science.sciencemag.org/content/315/5814/972."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Affinity Propagation seems similar to K-Means, but instead of clustering and then re-iterating, the algorithm sends messages from data to other data to figure out what's close and what's not. K-Means, on the other hand, chooses random centroids (not the case in AP) and then figures out which points are closest. Info from here: http://www.psi.toronto.edu/affinitypropagation/faq.html."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another super important (and helpful) feature of Affinity Propagation is that we don't need to specify the number of centroids / exemplars, which is key for the nature of our data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AffinityPropagation(affinity='precomputed', convergence_iter=15, copy=True,\n",
       "          damping=0.5, max_iter=200, preference=None, verbose=False)"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "affprop = sklearn.cluster.AffinityPropagation(affinity=\"precomputed\", damping = 0.5)\n",
    "affprop.fit(lev_similarity)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we fit the model, let's print out all of the clusters into a Pandas series (so we can index by a string)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def orderClusters(array):\n",
    "    \n",
    "    clusters = pd.Series()\n",
    "    \n",
    "    for cluster_id in np.unique(affprop.labels_):\n",
    "\n",
    "        exemplar = array[affprop.cluster_centers_indices_[cluster_id]]\n",
    "\n",
    "        cluster = np.unique(array[np.nonzero(affprop.labels_==cluster_id)])\n",
    "\n",
    "        if exemplar not in clusters:\n",
    "            clusters[exemplar] = cluster\n",
    "            \n",
    "    return clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "software engineer (software development director) :  ['software engineer (software development director)']\n",
      "vice president & chief operating officer :  ['vice president & chief operating officer'\n",
      " \"vice president, int'l compensation & benefits\"]\n",
      "physician resident :  ['industrial designer-automotive' 'mathematician and math modeler'\n",
      " 'pgy 4 medical resident/fellow ' 'physical therapy assistant' 'physician '\n",
      " 'physician resident' 'physician, internal medicine' 'physician/internist'\n",
      " 'production designer']\n",
      "postdoctoral fellow :  ['post doctoral fellow' 'post doctoral fellow in zoology'\n",
      " 'postdoctoral appointee' 'postdoctoral fellow' 'postdoctoral scholar'\n",
      " 'postdoctoral scientist' 'postgraduate researcher iv']\n",
      "computer support specialist :  ['computer specialist' 'computer support specialist'\n",
      " 'tecnical support specialist' 'title computer support specialist']\n",
      "dental office manager and dental assistant :  ['dental office manager and dental assistant'\n",
      " 'hotel culinary manager pastry (assistant)']\n",
      "progammer analyst :  ['budget analyst' 'derivatives analyst' 'mangement analyst'\n",
      " 'market research analyst' 'natural polymer chemist' 'progammer analyst'\n",
      " 'programmer analyst, level 2 ' 'programmer/analyst' 'programming analyst'\n",
      " 'sr. product support analyst' 'sr. systems analyst']\n",
      "computer systems administrator :  ['computer science instructor' 'computer systems administrator'\n",
      " 'computer systems analyst/programmer' 'senior network administrator'\n",
      " 'senior systems administrator']\n",
      "account exexcutive :  ['account executive i' 'account exexcutive' 'accountant/bursar']\n",
      "vice president/investment strategist uk equities :  ['vice president/investment strategist uk equities']\n"
     ]
    }
   ],
   "source": [
    "clusters = orderClusters(jobs_array)\n",
    "\n",
    "counter = 0\n",
    "for key in clusters.keys():\n",
    "    print key , ': ' , clusters[key]\n",
    "    counter += 1\n",
    "    if counter >= 10:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Definitely not bad for a first run through! A lot of these make sense and probably caught some real errors, like, at the bottom, \"account executive i\" and \"account exexcutive.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we adjust the \"preference\" argument, we can force the algorithm to employ more clusters. To see how changing the preference changes our cluster size, we'll run it a few different times to find the length of the dictionary (i.e. the number of exemplars)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "179"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "affprop = sklearn.cluster.AffinityPropagation(affinity=\"precomputed\", damping = 0.5, preference = -10)\n",
    "affprop.fit(lev_similarity)\n",
    "clusters = orderClusters(jobs_array)\n",
    "len(clusters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "291"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "affprop = sklearn.cluster.AffinityPropagation(affinity=\"precomputed\", damping = 0.5, preference = -2)\n",
    "affprop.fit(lev_similarity)\n",
    "clusters = orderClusters(jobs_array)\n",
    "len(clusters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's print out all of the clusters that have more than one member."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "postdoctoral fellow :  ['post doctoral fellow' 'postdoctoral fellow' 'postdoctoral scholar']\n",
      "architect  :  ['architect' 'architect ']\n",
      "application engineer :  ['application engineer' 'application engineer i']\n",
      "software design engineer/level 62 :  ['software design engineer/level 62' 'software design engineer/level 64']\n",
      "development systems architect :  ['development system architect' 'development systems architect']\n",
      "systems/software engineer :  ['systems software engineer' 'systems/software engineer']\n"
     ]
    }
   ],
   "source": [
    "clusters = orderClusters(jobs_array)\n",
    "\n",
    "counter = 0\n",
    "for key in clusters.keys():\n",
    "    if len(clusters[key]) >= 2:\n",
    "        print key , ': ' , clusters[key]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The total number of data points that we grabbed is close to being reached, so we can see that as the preference approaches 0, we approach no clusters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So how the heck are we supposed to know how many clusters are correct = what preference to use? Well, that's a great question, and the subject of this exact research paper from Cornell: https://arxiv.org/abs/0805.1096. The basic idea is – keep iterating until you converge on the right amount of clusters. It's called \"Adaptive Affinity Propagation.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Until the scientists figure that out, let's take our clusters and update the dataframe by replacing any values in the cluster with the exemplars."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
