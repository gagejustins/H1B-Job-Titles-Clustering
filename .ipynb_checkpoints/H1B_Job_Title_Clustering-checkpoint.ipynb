{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clustering to clean H1B Job Titles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal of this project is to try to replicate the clustering functionality of Google's OpenRefine software. The idea is that in some data fields, unstructured entries that are spelled differently, etc., may really mean the same thing. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's import the necessary packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import sklearn.cluster\n",
    "import distance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data is taken from governmental records of applications for HB1 visas. You can find it here: https://nyu.app.box.com/s/9oz3qx886zpwwfm6ewj89pvjuee2eqp5. I saved it with a csv extension in Sublime."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's load the data into a dataframe so we can steal the column we want."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SUBMITTED_DATE           object\n",
       "CASE_NO                  object\n",
       "NAME                     object\n",
       "ADDRESS                  object\n",
       "ADDRESS2                 object\n",
       "CITY                     object\n",
       "STATE                    object\n",
       "POSTAL_CODE              object\n",
       "NBR_IMMIGRANTS            int64\n",
       "BEGIN_DATE               object\n",
       "END_DATE                 object\n",
       "JOB_TITLE                object\n",
       "DOL_DECISION_DATE        object\n",
       "CERTIFIED_BEGIN_DATE     object\n",
       "CERTIFIED_END_DATE       object\n",
       "JOB_CODE                  int64\n",
       "APPROVAL_STATUS          object\n",
       "WAGE_RATE_1             float64\n",
       "RATE_PER_1               object\n",
       "MAX_RATE_1              float64\n",
       "PART_TIME_1              object\n",
       "CITY_1                   object\n",
       "STATE_1                  object\n",
       "PREVAILING_WAGE_1       float64\n",
       "WAGE_SOURCE_1            object\n",
       "YR_SOURCE_PUB_1         float64\n",
       "OTHER_WAGE_SOURCE_1      object\n",
       "WAGE_RATE_2             float64\n",
       "RATE_PER_2               object\n",
       "MAX_RATE_2              float64\n",
       "PART_TIME_2              object\n",
       "CITY_2                   object\n",
       "STATE_2                  object\n",
       "PREVAILING_WAGE_2       float64\n",
       "WAGE_SOURCE_2            object\n",
       "YR_SOURCE_PUB_2         float64\n",
       "OTHER_WAGE_SOURCE_2      object\n",
       "dtype: object"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"H1B.csv\")\n",
    "df.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're going to be working with job titles. First, let's take a look at what our job titles look like so we can understand the problem. We'll group the dataframe by titles, and then extract each one to a numpy array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "titles = df.groupby('JOB_TITLE')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "jobs = []\n",
    "counter = 0\n",
    "\n",
    "for group in titles.groups:\n",
    "    if group not in jobs:\n",
    "        jobs.append(group)\n",
    "        counter += 1\n",
    "    if counter >= 300:\n",
    "        break\n",
    "\n",
    "jobs_array = np.asarray(jobs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Software Engineer (Consultant)',\n",
       "       'Software Engineer (Software Development Director)',\n",
       "       'Assistant VP - Economist',\n",
       "       'VICE PRESIDENT & CHIEF OPERATING OFFICER', 'PHYSICIAN RESIDENT',\n",
       "       'Network Manager', 'IT Architect', 'BUSINESS DEVELOPMENT MANAGER ',\n",
       "       'PostDoctoral Fellow', 'Adjunct Trainer',\n",
       "       'PATENT SPECIALIST(Chemical Arts)', 'Staff Research Associate',\n",
       "       'SR. FINANCIAL TECHNOLOGY ADVISOR', 'Software Project Engineer',\n",
       "       'COMPUTER SUPPORT SPECIALIST', 'PGY 4 Medical Resident/Fellow ',\n",
       "       'DENTAL OFFICE MANAGER AND DENTAL ASSISTANT', 'PROGAMMER ANALYST',\n",
       "       'Web Applications Developer', 'Computer Systems Administrator'], \n",
       "      dtype='|S50')"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jobs_array[:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see if you scroll through the list, even once we've taken the unique titles out of the dataframe, there are tons of overlapping positions. There are lower case and upper case, words switched around, misspellings, etc. If we want to make this data useful and visualize it, we'll need to clean this up."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we can change all of the terms to lowercase. We can argue that it's a good idea to keep the punctuation, but we'll remove it to make it easier on the clustering later on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Convert all titles to lower case\n",
    "\n",
    "for i in range(len(jobs_array)):\n",
    "    jobs_array[i] = jobs_array[i].lower()\n",
    "\n",
    "#Strip punctuation\n",
    "\n",
    "for i in range(len(jobs_array)):\n",
    "    jobs_array[i] = jobs_array[i].strip('/.,:;-â€“')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, now our data is ready for clustering."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The way to calculate the similarity between strings is called the \"Levenshtein Distance.\" Code borrowed from http://stats.stackexchange.com/questions/123060/clustering-a-long-list-of-strings-words-into-similarity-groups. More info on the distance formula here: https://rosettacode.org/wiki/Levenshtein_distance#Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "lev_similarity = -1 * np.array([[distance.levenshtein(j1,j2) for j1 in jobs_array] for j2 in jobs_array])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we've created a matrix (in array form) of the Levenshtein Distance of each job title from the other job titles in the original jobs array. Here's what it looks like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  0, -25, -23, ..., -19, -18, -28],\n",
       "       [-25,   0, -40, ..., -38, -36, -37],\n",
       "       [-23, -40,   0, ..., -17, -18, -22],\n",
       "       ..., \n",
       "       [-19, -38, -17, ...,   0, -12, -24],\n",
       "       [-18, -36, -18, ..., -12,   0, -25],\n",
       "       [-28, -37, -22, ..., -24, -25,   0]])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lev_similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we'll cluster these values. Affinity Propagation seems to be the right algorithm for the job, since we've already calculated the Levenshtein Distances for our jobs array. The algorithm was first proposed for this purpose here:http://science.sciencemag.org/content/315/5814/972."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Affinity Propagation seems similar to K-Means, but instead of clustering and then re-iterating, the algorithm sends messages from data to other data to figure out what's close and what's not. K-Means, on the other hand, chooses random centroids (not the case in AP) and then figures out which points are closest. Info from here: http://www.psi.toronto.edu/affinitypropagation/faq.html."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another super important (and helpful) feature of Affinity Propagation is that we don't need to specify the number of centroids / exemplars, which is key for the nature of our data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AffinityPropagation(affinity='precomputed', convergence_iter=15, copy=True,\n",
       "          damping=0.5, max_iter=200, preference=None, verbose=False)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "affprop = sklearn.cluster.AffinityPropagation(affinity=\"precomputed\", damping = 0.5)\n",
    "affprop.fit(lev_similarity)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we fit the model, let's print out all of the clusters into a dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def orderClusters(array):\n",
    "    \n",
    "    clusters = {}\n",
    "    \n",
    "    for cluster_id in np.unique(affprop.labels_):\n",
    "\n",
    "        exemplar = array[affprop.cluster_centers_indices_[cluster_id]]\n",
    "\n",
    "        cluster = np.unique(array[np.nonzero(affprop.labels_==cluster_id)])\n",
    "\n",
    "        if exemplar not in clusters:\n",
    "            clusters[exemplar] = cluster\n",
    "            \n",
    "    return clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gallery coordinator :  ['gallery coordinator']\n",
      "it business systems analyst :  ['it business systems analyst']\n",
      "endocrine biologist :  ['endocrine biologist']\n",
      "web manager :  ['hotel manager' 'web manager']\n",
      "portfolio analyst risk management :  ['portfolio analyst risk management']\n",
      "software engineer - systems :  ['software engineer - systems']\n",
      "real estate appraisal management consultant :  ['real estate appraisal management consultant']\n",
      "sales account manager & specialist :  ['sales account manager & specialist']\n",
      "staff member :  ['staff member']\n",
      "physician  :  ['physician ']\n"
     ]
    }
   ],
   "source": [
    "clusters = orderClusters(jobs_array)\n",
    "\n",
    "#Print out the first 10 items\n",
    "\n",
    "counter = 0\n",
    "for key in clusters.keys():\n",
    "    print key , \": \" , clusters[key]\n",
    "    counter += 1\n",
    "    if counter >= 10:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Definitely not bad for a first run through! Starting from the top, a lot of these make sense. The first key is 'Account Executive', and the cluster includes values like 'Accountant/Bursar', which is indeed similar."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we adjust the \"preference\" argument, we can force the algorithm to employ more clusters. To see how changing the preference changes our cluster size, we'll run it a few different times to find the length of the dictionary (i.e. the number of exemplars)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "179"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "affprop = sklearn.cluster.AffinityPropagation(affinity=\"precomputed\", damping = 0.5, preference = -10)\n",
    "affprop.fit(lev_similarity)\n",
    "clusters = orderClusters(jobs_array)\n",
    "len(clusters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "269"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "affprop = sklearn.cluster.AffinityPropagation(affinity=\"precomputed\", damping = 0.5, preference = -5)\n",
    "affprop.fit(lev_similarity)\n",
    "clusters = orderClusters(jobs_array)\n",
    "len(clusters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The total number of data points that we grabbed is close to being reached, so we can see that as the preference approaches 0, we approach no clusters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So how the heck are we supposed to know how many clusters are correct = what preference to use? Well, that's a great question, and the subject of this exact research paper from Cornell: https://arxiv.org/abs/0805.1096. The basic idea is â€“ keep iterating until you converge on the right amount of clusters. It's called \"Adaptive Affinity Propagation.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "The question is â€“ how can we define a measure for the \"right\" amount of clusters? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
